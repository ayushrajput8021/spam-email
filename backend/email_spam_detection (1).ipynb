{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "624ZM8jnXOyr",
        "outputId": "66949b32-ec6d-4d5c-b1a9-3932a4950d83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.49.0\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMuPz-g2YFds",
        "outputId": "4398d2d7-eb18-4b88-d45a-810a34a0631f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, DistilBertConfig\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "# Check for MPS availability and set device\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Clear MPS cache to free up memory\n",
        "if device.type == \"mps\":\n",
        "    torch.mps.empty_cache()\n",
        "\n",
        "# Load and prepare data\n",
        "df_master = pd.read_csv('master_data (1).csv')\n",
        "df_converted = df_master.copy()\n",
        "df_converted[\"Category\"] = df_converted[\"Category\"].map({\"ham\": 0, \"spam\": 1})\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df_converted[\"Message\"], df_converted[\"Category\"], test_size=0.2, random_state=42, stratify=df_converted[\"Category\"]\n",
        ")\n",
        "\n",
        "# Load tokenizer and encode data\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  # Using a smaller model\n",
        "\n",
        "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True)\n",
        "\n",
        "# Define custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}  # Move tensors to MPS\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx]).to(device)  # Move labels to MPS\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, train_labels.tolist())\n",
        "val_dataset = CustomDataset(val_encodings, val_labels.tolist())\n",
        "\n",
        "# Configure model with higher dropout\n",
        "config = DistilBertConfig.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2,\n",
        "    dropout=0.3,  # default is 0.1\n",
        "    attention_dropout=0.3  # default is 0.1\n",
        ")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", config=config)\n",
        "model.to(device)  # Move model to MPS device\n",
        "\n",
        "# Compute metrics function\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZfidLo9qpDb",
        "outputId": "0d923ad0-69b8-433e-d6f3-bbe8a2ac9a5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in ./venv/lib/python3.13/site-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./venv/lib/python3.13/site-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.13/site-packages (from transformers) (2.2.4)\n",
            "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.13/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->transformers) (2025.1.31)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzHRdPKPq7Ea",
        "outputId": "fde764a8-110f-423d-85c8-c18d2f5d33a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.49.0\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "oRqD5Bs3qAz7",
        "outputId": "b649c463-42f7-4fd4-a4a4-41ecc84f7a56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ayush/Documents/atif-ai/venv/lib/python3.13/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  if self.no_cuda:\n"
          ]
        }
      ],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False\n",
        ")\n",
        "\n",
        "# Trainer with early stopping\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-iqOBl1WqDg_"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1016' max='1016' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1016/1016 44:17, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.144500</td>\n",
              "      <td>0.179022</td>\n",
              "      <td>0.952709</td>\n",
              "      <td>0.950579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.040500</td>\n",
              "      <td>0.080112</td>\n",
              "      <td>0.977833</td>\n",
              "      <td>0.977683</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 01:17]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance on Eval set: {'eval_loss': 0.08011189103126526, 'eval_accuracy': 0.9778325123152709, 'eval_f1': 0.9776829017898694, 'eval_runtime': 80.2219, 'eval_samples_per_second': 25.305, 'eval_steps_per_second': 0.399, 'epoch': 2.0}\n",
            "Model saved successfully to ./saved_modelv6\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate\n",
        "trainer.train()\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Performance on Eval set: {eval_results}\")\n",
        "\n",
        "# Save the model after training\n",
        "save_directory = \"./saved_modelv6\"\n",
        "trainer.save_model(save_directory)  # Save model, config, and tokenizer (if passed)\n",
        "print(f\"Model saved successfully to {save_directory}\")\n",
        "\n",
        "# Clear MPS cache after training\n",
        "if device.type == \"mps\":\n",
        "    torch.mps.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
